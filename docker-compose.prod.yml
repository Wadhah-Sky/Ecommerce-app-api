# Define the version of docker compose.

# Note: for production environment we expose (using ports variable) only the services that we want
#       to be reachable from host machine (e.g, cloud node machine)

version: "3.9"

services:
  cache:
    image: redis:7-alpine
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.30'
          memory: 300M
        reservations:
          cpus: '0.05'
          memory: 20M
    # ports:
    #  - '6379:6379'
    expose:
      - '6379'
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep 'PONG' || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 20s
      retries: 3
    networks:
      - webnet

  db:
    image: 127.0.0.1:5000/postgres
    build:
      context: .
      dockerfile: compose/local/postgres/Dockerfile
      target: prod
    # ports:
    #  - '5432:5432'
    expose:
      - '5432'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready --quiet --dbname=$${POSTGRES_DB} --username=$${POSTGRES_USER} || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 30s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes:
     - postgres-db:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${SQL_NAME}
      - POSTGRES_USER=${SQL_USER}
      - POSTGRES_PASSWORD=${SQL_PASS}
    networks:
      - webnet

  pgadmin:
    image: dpage/pgadmin4
    healthcheck:
      test: ["CMD", "wget", "-O", "-", "http://localhost:80/misc/ping"]
      interval: 10s
      timeout: 10s
      start_period: 160s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.50'
          memory: 700M
        reservations:
          cpus: '0.05'
          memory: 20M
    # ports:
    #  - '5050:80'
    expose:
      - '5050'
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD}
    depends_on:
      - db
    networks:
      - webnet

  elasticsearch:
    image: 127.0.0.1:5000/elasticsearch
    build:
      context: .
      dockerfile: compose/local/elk/elasticsearch/Dockerfile
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    # ports:
    #  - "9200:9200"
    #  - "9300:9300"
    expose:
      - '9200'
      - '9300'
    healthcheck:
      test: [ "CMD-SHELL", "curl -u elastic:${ELASTIC_PASSWORD} localhost:9200/_cluster/health?pretty", " | grep -E ", '"status" : "yellow"', " || exit 1" ]
      interval: 30s
      timeout: 10s
      start_period: 240s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.40'
          memory: 700M
        reservations:
          cpus: '0.05'
          memory: 50M
    volumes:
      - ./compose/local/elk/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z
      - elasticsearch:/usr/share/elasticsearch/data:Z
    environment:
      - node.name=${ELASTICSEARCH_NODE_NAME}
      - ES_JAVA_OPTS=${ES_JAVA_OPTS}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-}
      - discovery.type=${DISCOVERY_TYPE}
    networks:
      - webnet

  elk_setup:
    profiles:
      - setup
    image: 127.0.0.1:5000/elk-setup
    build:
      context: .
      dockerfile: compose/local/elk/setup/Dockerfile
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    init: true
    volumes:
      - ./compose/local/elk/setup/entrypoint.sh:/entrypoint.sh:ro,Z
      - ./compose/local/elk/setup/lib.sh:/lib.sh:ro,Z
      - ./compose/local/elk/setup/roles:/roles:ro,Z
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - LOGSTASH_INTERNAL_PASSWORD=${LOGSTASH_INTERNAL_PASSWORD}
      - KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD}
      - METRICBEAT_INTERNAL_PASSWORD=${METRICBEAT_INTERNAL_PASSWORD}
      - FILEBEAT_INTERNAL_PASSWORD=${FILEBEAT_INTERNAL_PASSWORD}
      - HEARTBEAT_INTERNAL_PASSWORD=${HEARTBEAT_INTERNAL_PASSWORD}
      - MONITORING_INTERNAL_PASSWORD=${MONITORING_INTERNAL_PASSWORD}
      - BEATS_SYSTEM_PASSWORD=${BEATS_SYSTEM_PASSWORD}
    depends_on:
      - elasticsearch
    networks:
      - webnet

  app:
    image: 127.0.0.1:5000/django
    build: &app_build
      context: .
      dockerfile: compose/local/django/Dockerfile
      target: prod
    # Expose ports without publishing them to the host machine - theyâ€™ll only be accessible
    # to linked services. Only the internal port can be specified.
    # Note: expose means the ports are not exposed to external, only exposed to other docker services.
    #       we change to expose in django service because this service will be accessible by Nginx
    #       service.
    expose:
      - '8000'
    healthcheck:
      test: ["CMD-SHELL", "curl --fail --silent --output /dev/null http://localhost:8000/ping || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 60s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    # Note: The static files are not being loaded anymore. This is expected since Debug mode is off, so
    #       django browsable api will not show in style, can be fixed it with Nginx that loading static files
    #       by share common volume between django service and nginx.
    volumes:
      # - ./backend/django_rest:/usr/src/backend/django_rest
      # - ./backend/django_rest/data:/usr/src/vol/web
      - static_volume:/usr/src/vol/web/static
      - media_volume:/usr/src/vol/web/media
    command: bash -c "/usr/src/compose/start-django.sh"
    environment: &app_env
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=${DEBUG}
      - ALLOWED_HOSTS=${ALLOWED_HOSTS}
      - USE_X_FORWARDED_HOST=${USE_X_FORWARDED_HOST}
      - CSRF_TRUSTED_ORIGINS=${CSRF_TRUSTED_ORIGINS}
      - CELERY_BROKER=${CELERY_BROKER}
      - CELERY_BACKEND=${CELERY_BACKEND}
      - SQL_ENGINE=${SQL_ENGINE}
      - SQL_HOST=${SQL_HOST}
      - SQL_NAME=${SQL_NAME}
      - SQL_USER=${SQL_USER}
      - SQL_PASS=${SQL_PASS}
      - SQL_PORT=5432
      - EMAIL_BACKEND_NAME=${EMAIL_BACKEND_NAME}
      - EMAIL_BACKEND=${EMAIL_BACKEND}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - MAILJET_API_KEY=${MAILJET_API_KEY}
      - MAILJET_SECRET_KEY=${MAILJET_SECRET_KEY}
      - ELASTIC_USERNAME=${ELASTIC_USERNAME}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTICSEARCH_HOSTS=${ELASTICSEARCH_HOSTS}
      - DJANGO_SUPERUSER_EMAIL=${DJANGO_SUPERUSER_EMAIL}
      - DJANGO_SUPERUSER_USERNAME=${DJANGO_SUPERUSER_USERNAME}
      - DJANGO_SUPERUSER_PHONE_NUMBER=${DJANGO_SUPERUSER_PHONE_NUMBER}
      - DJANGO_SUPERUSER_FIRST_NAME=${DJANGO_SUPERUSER_FIRST_NAME}
      - DJANGO_SUPERUSER_LAST_NAME=${DJANGO_SUPERUSER_LAST_NAME}
      - DJANGO_SUPERUSER_PASSWORD=${DJANGO_SUPERUSER_PASSWORD}
    depends_on:
      - db
      - cache
      - elasticsearch
    networks:
      - webnet

  worker:
    image: 127.0.0.1:5000/worker
    build: *app_build
    healthcheck:
      test: [ "CMD-SHELL", "celery inspect ping || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 100s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes: &worker_vol
      - ./backend/django_rest:/usr/src/backend/django_rest
    command: bash -c "/usr/src/compose/start-celeryworker.sh"
    environment: *app_env
    depends_on:
      - app
    networks:
      - webnet

  nginx:
    # This service will use deploy stage in Vue docker file where copy the contains of 'dist' directory
    # from build process and paste it into 'html' directory of Nginx image.
    image: 127.0.0.1:5000/nginx
    build:
      context: .
      dockerfile: compose/local/vue/Dockerfile
      target: deploy
    ports:
      # The ports have been listed (on left side) means you can reach the container using 80, 433
      # while the ports on right side will listen to them by host (Nginx) server which in turn redirect
      # HTTP request to upstream server.
      # Info: in frontend browser, the domain:80 is different from domain:433 and each one has own local
      #       storage.
      - "80:80"
      - "443:443"
    healthcheck:
      # Test will return 0 if /health url response with 200.
      # Note: /health is location url that we set in 'default.conf' file of Nginx image.
      test: [ "CMD-SHELL", "curl -s -I http://localhost:80/health | grep -q 'HTTP/1.1 200 OK' || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 60s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes:
      # Nginx shares the same volume that connected to static files which being served in django service (app)
      - static_volume:/usr/src/vol/web/static
      - media_volume:/usr/src/vol/web/media
      # /var/www/certbot/ is already exists in the image so no need to create it.
      - certbot-www:/var/www/certbot
      # You can point volume to file because you will get an error is that file a directory?
      - letsencrypt:/usr/share/nginx/certificates/
    command: bash -c "/usr/src/compose/start-nginx.sh"
    environment:
      - CERT_DOMAINS=${CERT_DOMAINS} # example.com
      - CERT_EMAIL=${CERT_EMAIL} # Your email
      - CERT_GENERATE=${CERT_GENERATE:-0} # default is 0 (means false)
      - CERT_TEST_CERT=${CERT_TEST_CERT}
      - LETSENCRYPT_DIR=${LETSENCRYPT_DIR}
      - CERT_NAME=${CERT_NAME}
      - CERT_RENEW_INTERVAL=${CERT_RENEW_INTERVAL}
      - CUSTOM_CERT_COMMAND=${CUSTOM_CERT_COMMAND} # if set, will have the priority to be executed.
    depends_on:
      - app
    networks:
      - webnet

# Set network that all services will be in it.
networks:
  webnet:

volumes:
  postgres-db:
    driver: local
  elasticsearch:
    driver: local
  static_volume:
    driver: local
  media_volume:
    driver: local
  certbot-www:
    driver: local
  letsencrypt:
    driver: local