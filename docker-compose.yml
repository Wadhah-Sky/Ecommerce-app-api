---

# Specify the configuration for your application docker compose.

# Note: if you have multiple docker compose .yml file in the directory the system currently in,
#       then you can use -f flag (file path):
#
#       >> docker-compose -f docker-compose.yml up
#
#       also:
#
#       >> docker-compose -f docker-compose.yml up <specific_service_name>

# Note: if you specify only the image variable for the service, then make sure it's the source for the image.
#       And if you set both of image and build variables, so it's mean the image variable will represent the
#       name for image that will be build in the registry.
#       it's very crucial to set both of variables if working with Docker swarm environment if using Dockerfile.

# Remember: the health check test and the deploy option is use in swarm mode, and the health test is run by
#           service itself not by daemon which mean you should use localhost ip in the commands NOT the service
#           name.

# Info: If you have multiple environments, you may want to look at using a
#       docker-compose.override.yml configuration file. With this approach,
#       you'd add your base config to a docker-compose.yml file and then use
#       a docker-compose.override.yml file to override those config settings
#       based on the environment.

# Define the version of docker compose.
version: "3.9"

# Define the services that make up the project application.
services:

  # Name of the service (container).
  cache:
    # Specify image for this container to install.
    image: redis:7-alpine

    # cpus and memory constraints: use with version 2.0 of Docker compose.
    # mem_limit: 300m
    # mem_reservation: 100m

    # Version 3.0 to configure resource constraints, will work within Docker swarm environment if available.
    deploy:
      # Specify the number of replicas of this service to be created in the swarm.
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          # 0.30 means 30% of overall available single core of cpu (if you have cpu with two cores then
          # its overall is 2)
          cpus: '0.30'
          memory: 200M
        reservations:
          cpus: '0.05'
          memory: 20M

    # Specify restart flag to unless-stopped, so when the container is stopped (manually or otherwise),
    # it is not restarted even after Docker daemon restarts.
    # Note: the default value is 'no', Do not automatically restart the container.
    # IMPORTANT: this variable will be ignored if push the service into Docker swarm which have 'restart_policy'
    #            in 'deploy' variable.
    #restart: unless-stopped

    # Map the TCP port from Host to Container.
    ports:
      - '6379:6379'

    # If you're using an orchestration tool other than Docker Swarm -- i.e.,
    # Kubernetes or AWS ECS -- it's highly likely that the tool has its own
    # internal system for handling health checks.
    healthcheck:
      # For 'redis' service we use 'redis-cli' tool with 'ping' command to check the output is 'PONG'
      # which mean redis is live, otherwise exit.
      # 'grep' tool stands for global search for regular expression and print out, The grep filter searches
      # a file for a particular pattern of characters.
      # We pass the output of 'redis-cli ping' by using pip operator to the 'grep' tool.
      # OR (||) operator will see if left side is fail then execute the right command which will exit.
      test: ["CMD-SHELL", "redis-cli ping | grep 'PONG' || exit 1"]

      # specify the time between the health check for the application container. it waits for the
      # specified time from one check to another.
      interval: 10s

      # If a single health check takes longer than the time defined in the 'timeout' that run will be considered a failure.
      timeout: 10s

      # specify the number of seconds the container needs to start; health check will wait for that time to start.
      start_period: 20s

      # specify the number of consecutive health check failures required to declare the container status as unhealthy.
      retries: 3

    networks:
      - webnet-dev

  db:
    # In Docker swarm environment, it's very important to name image that build using dockerfile as:
    # registry_URL:target_port/name_for_image
    image: 127.0.0.1:5000/postgres-dev
    # Building configuration of this service.
    # ampersand mark means this service build config will be used by other services to build.
    build:
      # Specify the context scope for this service to pick up related Dockerfile, could be a 'URL'.
      context: .
      # Specify Dockerfile.
      dockerfile: compose/local/postgres/Dockerfile
      # Specify the stage of Dockerfile that 'docker-compose build' will stop at.
      target: dev
    ports:
      - '5432:5432'
    healthcheck:
      # By using 'pg_isready' tool we can check if a specific database for specific user is exists and running.
      # --quite: is option tell this tool, don't print out the response script.
      # the $${env_var_name} means use env variables from the container env system, if you run same command from
      # inside the container then only use ${env_var_name}.
      # Note: if you don't set the --username value then the 'pg_ready' will use by default the current user of
      # container( default is 'root' unless if you change it ) which will print:
      #
      # Fatal Error: <user_name> is not exist.
      #
      test: ["CMD-SHELL", "pg_isready --quiet --dbname=$${POSTGRES_DB} --username=$${POSTGRES_USER} || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 30s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    # Map a specific directory on container with a directory in the project directory on the local machine.
    #
    # Note: the postgres 'initdb' command that will create the databases cluster in the container need to do
    # some 'chmod' commands for the directory that hold cluster, and this can make an error:
    #
    # The files belonging to this database system will be owned by user "user". This user must also own the server process.
    # initdb: error: could not access directory "/var/lib/postgresql/data": Permission denied
    #
    # You will face this error if trying to mount a volume from host machine with the 'data' directory in the container
    # where 'initdb' command will/created the databases cluster, because this directory has limited permissions, due
    # the user of the container is not the same of the one who in PostgresSQL database and this the reason for the error.
    volumes:
     - postgres-db-dev:/var/lib/postgresql/data
    # Set environment variables to be use inside container, if there are already such variables in the image that will
    # use to create the container, then these variables will overwrite the ones of the image.
    environment:
      - POSTGRES_DB=${SQL_NAME}
      - POSTGRES_USER=${SQL_USER}
      - POSTGRES_PASSWORD=${SQL_PASS}
    # Set the network that this container will be in it.
    networks:
      - webnet-dev

  pgadmin:
    image: dpage/pgadmin4
    healthcheck:
      # the following ping test (comment) not working in swarm mode.
      # test: ["CMD", "ping localhost:80 || exit 1"]
      test: ["CMD", "wget", "-O", "-", "http://localhost:80/misc/ping"]
      interval: 10s
      timeout: 10s
      # Note: in pgAdmin container, the 'start_period' variable should be at least
      #       140s otherwise will face error of:
      #       worker pid [<number>] has terminated due signal 9
      start_period: 160s
      retries: 3
    deploy:
      # Specify the number of replicas of this service to be created in the swarm.
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.50'
          memory: 700M
        reservations:
          cpus: '0.05'
          memory: 20M
    ports:
      - '5050:80'
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD}
    depends_on:
      - db
    networks:
      - webnet-dev

  elasticsearch:
    # In Docker swarm environment, it's very important to name image that build using dockerfile as:
    # registry_URL:target_port/name_for_image
    image: 127.0.0.1:5000/elasticsearch-dev
    build:
      context: .
      dockerfile: compose/local/elk/elasticsearch/Dockerfile
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    ports:
      - '9200:9200'
      - '9300:9300'
    healthcheck:
      # Here our test is not about starting the service, it's about the Elasticsearch engine is ready and run
      # in another means http response of engine health is containing "status" : "yellow" or "status" : "green".
      # Note: if you want to use this command within this service container directly, then use 'localhost:9200' or
      #       'http://localhost:9200' but if your trying to connect from another service then use '<name_of_service:port>'
      #       because 'elasticsearch' is related to docker daemon that use names of containers to manage connection.
      # Important: 'grep' tool use single quotes for string that contains double quotes NOT using double quotes.
      # Info: The status "yellow" indicates that replica shards of that certain index could not get allocated to other nodes.
      #       This can happen for various reasons. For example:
      #       1- you have specified more replicas than you have nodes.
      #       2- It depends on your cluster setup and whether you configured shard allocation by your own or not.
      #       3- if you are running Elasticsearch with only 1 node, like when running Elasticsearch on local in
      #          a Docker container, and you only have 1 container. Elasticsearch will not make that node/container
      #          a replica, so your indices would always be yellow.
      test: [ "CMD-SHELL", "curl -u elastic:${ELASTIC_PASSWORD} localhost:9200/_cluster/health?pretty", " | grep -E ", '"status" : "yellow|green"', " || exit 1" ]
      interval: 30s
      timeout: 10s
      start_period: 240s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          # Note: be careful when specify the values due it's possible to cause to exit the container with 0
          #       at initializing time because health check test run before service is ready.
          cpus: '0.40'
          memory: 700M
        reservations:
          cpus: '0.05'
          memory: 50M
    volumes:
      - ./compose/local/elk/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z
      - elasticsearch-dev:/usr/share/elasticsearch/data:Z
    environment:
      - node.name=${ELASTICSEARCH_NODE_NAME}
      - ES_JAVA_OPTS=${ES_JAVA_OPTS}
      # Bootstrap password, used to initialize the keystore during the initial startup of Elasticsearch.
      # Ignored on subsequent runs.
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-}
      # Use single node discovery in order to disable production mode and avoid bootstrap checks.
      # see: https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html
      - discovery.type=${DISCOVERY_TYPE}
    networks:
      - webnet-dev

  elk_setup:
    # The 'setup' service runs a one-off script which initializes users inside
    # Elasticsearch — such as 'logstash_internal' and 'kibana_system' — with the
    # values of the passwords defined in the '.env' file. It also creates the
    # roles required by some of these users.
    #
    # This task only needs to be performed once, during the *initial* startup of
    # the stack (ELK services). Any subsequent run will reset the passwords of existing
    # users to the values defined inside the '.env' file, and the built-in roles to their
    # default permissions.
    #
    # By default, it is excluded from the services started by 'docker compose up'
    # due to the non-default profile it belongs to. To run it, either provide the
    # '--profile elk_setup' CLI flag to Compose commands which will up all services in
    # selected docker compose file:
    #
    # >> docker-compose --profile elk_setup up
    #
    # or "up" the service by name such as:
    #
    # >> docker-compose up elk_setup
    #
    profiles:
      #
      # Note: 'profiles' parameter/property is not allowed in Docker Swarm mode.
      # Profiles help you adjust the Compose application model for various uses and environments
      # by selectively starting services. This is achieved by assigning each service to zero or
      # more profiles. If unassigned, the service is always started but if assigned, it is only
      # started if the profile is activated. Another way to define profiles parameter:
      # profiles: ["setup"]
      #
      - setup
    image: 127.0.0.1:5000/elk-setup-dev
    build:
      context: .
      dockerfile: compose/local/elk/setup/Dockerfile
      args:
        # Pass arguments values that defined inside the selected Dockerfile.
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    # We can use the –init parameter as an option with the 'docker run --init' command to start
    # a container as the main process with PID 1. Furthermore, it starts and manages all the other
    # processes running inside the container. Or you can define a parameter 'init' to be true inside
    # docker compose file for certain service like below:
    init: true
    volumes:
      - ./compose/local/elk/setup/entrypoint.sh:/entrypoint.sh:ro,Z
      - ./compose/local/elk/setup/lib.sh:/lib.sh:ro,Z
      - ./compose/local/elk/setup/roles:/roles:ro,Z
    environment:
      # Note: we are here passing default value in case no access to environment variables, and should be at
      #       least 6 characters.
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - LOGSTASH_INTERNAL_PASSWORD=${LOGSTASH_INTERNAL_PASSWORD}
      - KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD}
      - METRICBEAT_INTERNAL_PASSWORD=${METRICBEAT_INTERNAL_PASSWORD}
      - FILEBEAT_INTERNAL_PASSWORD=${FILEBEAT_INTERNAL_PASSWORD}
      - HEARTBEAT_INTERNAL_PASSWORD=${HEARTBEAT_INTERNAL_PASSWORD}
      - MONITORING_INTERNAL_PASSWORD=${MONITORING_INTERNAL_PASSWORD}
      - BEATS_SYSTEM_PASSWORD=${BEATS_SYSTEM_PASSWORD}
    depends_on:
      - elasticsearch
    networks:
      - webnet-dev

  logstash:
    image: 127.0.0.1:5000/logstash-dev
    build:
      context: .
      dockerfile: compose/local/elk/logstash/Dockerfile
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - ./compose/local/elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro,Z
      - ./compose/local/elk/logstash/pipeline:/usr/share/logstash/pipeline:ro,Z
    ports:
      - '5044:5044'
      - '50000:50000/tcp'
      - '50000:50000/udp'
      - '9600:9600'
    healthcheck:
      test: [ "CMD-SHELL", "bin/logstash -t" ]
      interval: 10s
      timeout: 10s
      start_period: 180s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.40'
          memory: 700M
        reservations:
          cpus: '0.02'
          memory: 20M
    environment:
      - LS_JAVA_OPTS=${LS_JAVA_OPTS}
      - LOGSTASH_INTERNAL_PASSWORD=${LOGSTASH_INTERNAL_PASSWORD:-}
    depends_on: &logstash_depends
      - elasticsearch
    networks:
      - webnet-dev

  kibana:
    image: 127.0.0.1:5000/kibana-dev
    build:
      context: .
      dockerfile: compose/local/elk/kibana/Dockerfile
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    ports:
      - '5601:5601'
    healthcheck:
      test: [ "CMD-SHELL", "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'" ]
      interval: 10s
      timeout: 10s
      start_period: 180s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.40'
          memory: 700M
        reservations:
          cpus: '0.03'
          memory: 50M
    volumes:
      - ./compose/local/elk/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro,Z
    environment:
      - KIBANA_SYSTEM_PASSWORD=${KIBANA_SYSTEM_PASSWORD:-}
      - node.name=${KIBANA_NODE_NAME}
    depends_on: *logstash_depends
    networks:
      - webnet-dev

  app:
    image: 127.0.0.1:5000/django-dev
    # Building configuration of this service.
    # ampersand mark means this service build config will be used by other services to build.
    build: &app_build
      # Specify the context scope for this service to pick up related Dockerfile, could be a 'URL'.
      context: .
      # Specify Dockerfile.
      dockerfile: compose/local/django/Dockerfile
      # Specify the stage of Dockerfile that 'docker-compose build' will stop at.
      target: dev
    # Expose ports. Either specify both ports (HOST:CONTAINER), or just the container
    # port (a random host port will be chosen).
    # Note: 1- Ports mentioned in docker-compose.yml will be shared among different
    #          services started by the docker-compose.
    #       2- Ports will be exposed to the host machine to a random port or a given port.
    ports:
      - '8000:8000'
    healthcheck:
      # by using 'curl' network command tool, check if specific url is live(landing page will response) or not.
      # --fail: make sure (HTTP) Fail silently (no output at all).
      # --silent: make sure to not print the output of HTTP response.
      # --output: is used to specify where to hold the response HTTP document, you can find the response in
      #           '--show-error' text file in your container WORKDIR.
      test: ["CMD-SHELL", "curl --fail --silent --output /dev/null http://localhost:8000/ping || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 60s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes:
      - ./backend/django_rest:/usr/src/backend/django_rest
      - ./backend/django_rest/data:/usr/src/vol/web
    # Execute a specific commands or script file (.sh)
    # 'start-django.sh' is the shell script file used to run the service.
    # Always run your commands using sh/bash shell.
    command: bash -c "/usr/src/compose/start-django.sh"
    # Specify the environment variables file (.env) to copy into the container.
    # Note: don't use this way, better to set env variables manually due security reasons.
    #env_file:
    #  - .env
    # Note: integers environment variables, set them directly, since on tests server could face an issue
    # when trying to retrieve them from the cloud storage because they will be stored as string.
    environment: &app_env
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=${DEBUG}
      - MAIN_DOMAIN_NAME=${MAIN_DOMAIN_NAME}
      - ALLOWED_HOSTS=${ALLOWED_HOSTS}
      - CELERY_BROKER=${CELERY_BROKER}
      - CELERY_BACKEND=${CELERY_BACKEND}
      - SQL_ENGINE=${SQL_ENGINE}
      - SQL_HOST=${SQL_HOST}
      - SQL_NAME=${SQL_NAME}
      - SQL_USER=${SQL_USER}
      - SQL_PASS=${SQL_PASS}
      - SQL_PORT=5432
      - EMAIL_BACKEND_NAME=${EMAIL_BACKEND_NAME}
      - EMAIL_BACKEND=${EMAIL_BACKEND}
      - DEFAULT_FROM_EMAIL=${DEFAULT_FROM_EMAIL}
      - SERVER_EMAIL=${SERVER_EMAIL}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - MAILJET_API_KEY=${MAILJET_API_KEY}
      - MAILJET_SECRET_KEY=${MAILJET_SECRET_KEY}
      - SENDINBLUE_API_KEY=${SENDINBLUE_API_KEY}
      - ELASTIC_USERNAME=${ELASTIC_USERNAME}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTICSEARCH_HOSTS=${ELASTICSEARCH_HOSTS}
      - DJANGO_SUPERUSER_EMAIL=${DJANGO_SUPERUSER_EMAIL}
      - DJANGO_SUPERUSER_USERNAME=${DJANGO_SUPERUSER_USERNAME}
      - DJANGO_SUPERUSER_PHONE_NUMBER=${DJANGO_SUPERUSER_PHONE_NUMBER}
      - DJANGO_SUPERUSER_FIRST_NAME=${DJANGO_SUPERUSER_FIRST_NAME}
      - DJANGO_SUPERUSER_LAST_NAME=${DJANGO_SUPERUSER_LAST_NAME}
      - DJANGO_SUPERUSER_PASSWORD=${DJANGO_SUPERUSER_PASSWORD}
    depends_on:
      # In case you don't need to create customized commands in order to check fot service is ready and run,
      # Here we depend on health check (service_healthy).
      # Note: this way of depends_on is not supporting in swarm mode and will show the following error:
      #       services.app.depends_on must be a list
      # Important: elasticsearch engine should be running and ready ("status" : "green"), otherwise will
      #            face issue of connection [error 111]
      # Info: it's better to use list of depends_on services because it's too complicated to count right
      #       timer to start each srvice health test, so will lead to issues.
      # db:
      # condition: service_healthy
      # cache:
        # condition: service_healthy
      # elasticsearch:
        # condition: service_healthy
      - db
      - cache
      - elasticsearch
    networks:
      - webnet-dev

  ui:
    image: 127.0.0.1:5000/vue-dev
    build:
      context: .
      dockerfile: compose/local/vue/Dockerfile
      target: dev
    ports:
      - '8080:8080'
    healthcheck:
      # Since this service using linux Alpine image, then its has 'wget' network command tool.
      # we just need to check if server is up or not.
      # -q: is like quite which means don't print the output.
      # --spider <url>: to check if the specified url is exists or not, if exist then return 0 if not return 1.
      # Note: maybe you noticed we don't use http://localhost:8080/ping like what we do with django service,
      # because this is frontend and don't have /ping route or path, so will return 404 and exit.
      test: [ "CMD-SHELL", "wget -q --spider http://localhost:8080/ || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 80s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

    # Map the Vue project files between local machine and container to be updated whenever
    # a change done on one side, while only mount 'node_modules' folder as anonymous volume
    # to prevent overwritten by local machine directory at runtime, But this will cause an error
    # when trying to use dependencies like 'bootstrap-vue-3' or 'vuex' with Webpack server and will
    # show:
    #
    # Module not found: Error: Can't resolve 'vuex' in ....
    #
    # even if you are already install 'vuex', and this happens when using 'npm install' command on local machine
    # or using 'vue add' command to install plugins on container in order to update your project and then re-build
    # it and happen to be the plugins not installed correctly and this could lead to vue/cli not configured
    # correctly, so, to solve this issue mount your entire project with node_modules even if it will take more time
    # for building when create new container.
    volumes:
      - ./frontend/vue_spa:/usr/src/frontend/vue_spa
      - /usr/src/frontend/vue_spa/node_modules
    command: sh -c "npm run serve"
    environment:
      - CHOKIDAR_USEPOLLING=${CHOKIDAR_USEPOLLING}
    depends_on:
      - app
    networks:
      - webnet-dev

  worker:
    image: 127.0.0.1:5000/worker-dev
    # asterisk mark refer to another service same variable configs to be use in this service.
    build: *app_build
    healthcheck:
      # By using 'celery' command tool we check if there is response(live) from celery MainProcess, it sends a "ping" task
      # on the broker, workers respond and celery fetches the responses. which will return 'Pong' with details about how many
      # nodes (workers) are running.
      test: [ "CMD-SHELL", "celery inspect ping || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 100s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes: &worker_vol
      - ./backend/django_rest:/usr/src/backend/django_rest
    command: bash -c "/usr/src/compose/start-celeryworker.sh"
    environment: *app_env
    depends_on:
      - app
    networks:
      - webnet-dev

  beat:
    image: 127.0.0.1:5000/beat-dev
    build: *app_build
    healthcheck:
      test: ["CMD-SHELL", "celery inspect ping || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 120s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes: *worker_vol
    command: bash -c "/usr/src/compose/start-celerybeat.sh"
    environment: *app_env
    depends_on:
      - worker
    networks:
      - webnet-dev

  flower:
    image: 127.0.0.1:5000/flower-dev
    build: *app_build
    ports:
      - '5557:5555'
    healthcheck:
      test: [ "CMD-SHELL", "celery inspect ping || exit 1"]
      interval: 10s
      timeout: 10s
      start_period: 120s
      retries: 3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes: *worker_vol
    command: bash -c "/usr/src/compose/start-celeryflower.sh"
    environment: *app_env
    depends_on:
      - worker
    networks:
      - webnet-dev

# Set network that all services will be in it.
networks:
  webnet-dev:

volumes:
  postgres-db-dev:
    driver: local
  elasticsearch-dev:
    driver: local